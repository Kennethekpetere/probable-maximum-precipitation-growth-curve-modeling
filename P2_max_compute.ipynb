{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f996e85f-c1ae-4212-9f9d-111498142651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "from io import StringIO\n",
    "import urllib3\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cb53547-777f-4bed-bfcf-ed7c10c77151",
   "metadata": {},
   "source": [
    "#### **Annual Maximums**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "399d70b5-b9a4-4da6-ab50-8487075e7138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual maximums saved to annual_max.csv.\n"
     ]
    }
   ],
   "source": [
    "# function to extract annual maximums\n",
    "def get_annual_maximums(input_folder, output_csv):\n",
    "    durations = {\n",
    "        \"30-min\": 1,\n",
    "        \"1-hour\": 2,\n",
    "        \"2-hour\": 4,\n",
    "        \"3-hour\": 6,\n",
    "        \"6-hour\": 12,\n",
    "        \"12-hour\": 24,\n",
    "        \"24-hour\": 48,\n",
    "        \"48-hour\": 96,\n",
    "        \"72-hour\": 144\n",
    "    }\n",
    "\n",
    "    # Initialize the output DataFrame\n",
    "    output_data = []\n",
    "\n",
    "    # Process each file in the input folder\n",
    "    for file in os.listdir(input_folder):\n",
    "        if file.endswith(\".csv\"):\n",
    "            # Extract metadata from the filename\n",
    "            parts = file.split(\"_\")\n",
    "            station_id = parts[1]\n",
    "            latitude = float(parts[2])\n",
    "            longitude = float(parts[3].replace(\".csv\", \"\"))\n",
    "\n",
    "            # Read the file into a DataFrame\n",
    "            file_path = os.path.join(input_folder, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Convert the time column to datetime\n",
    "            df[\"time\"] = pd.to_datetime(df[\"time\"], errors='coerce')\n",
    "\n",
    "            # Ensure the data is sorted by time\n",
    "            df = df.sort_values(by=\"time\").reset_index(drop=True)\n",
    "\n",
    "            # Extract unique years from 2000 to 2024\n",
    "            df['year'] = df['time'].dt.year\n",
    "            years = range(2000, 2025)\n",
    "\n",
    "            # Calculate annual maximums for each year and duration\n",
    "            for year in years:\n",
    "                year_data = df[df['year'] == year]\n",
    "\n",
    "                if year_data.empty:\n",
    "                    continue\n",
    "\n",
    "                annual_max = {\n",
    "                    \"ID\": station_id,\n",
    "                    \"lat\": latitude,\n",
    "                    \"lon\": longitude,\n",
    "                    \"year\": year\n",
    "                }\n",
    "\n",
    "                # Compute the maximums for each duration\n",
    "                for duration, window in durations.items():\n",
    "                    if window == 1:\n",
    "                        # 30-min maximums directly from the data\n",
    "                        annual_max[duration] = year_data['precipitation'].max()\n",
    "                    else:\n",
    "                        # Calculate rolling sums for longer durations\n",
    "                        rolling_sums = year_data['precipitation'].rolling(window=window, min_periods=1).sum()\n",
    "                        annual_max[duration] = rolling_sums.max()\n",
    "\n",
    "                output_data.append(annual_max)\n",
    "\n",
    "    # Create a DataFrame from the results\n",
    "    output_df = pd.DataFrame(output_data)\n",
    "\n",
    "    # Save the results to the output CSV file\n",
    "    output_df.to_csv(output_csv, index=False)\n",
    "\n",
    "# input and output paths\n",
    "input_folder = \"output_filesv7\"\n",
    "output_csv = \"annual_max.csv\"\n",
    "\n",
    "# Run function\n",
    "get_annual_maximums(input_folder, output_csv)\n",
    "\n",
    "print(f\"Annual maximums saved to {output_csv}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e318bab-1a73-42d3-be3a-c8f6ab5ea2b3",
   "metadata": {},
   "source": [
    "#### **All-time Maximums (2000 to 2024) - Jude Approach**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7ac69f60-07bd-4cb8-afed-2a6e4541e557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All-time maximums saved to alltime_max.csv.\n"
     ]
    }
   ],
   "source": [
    "# Function to compute all time maximums from different storms (Jude's Idea)\n",
    "\n",
    "def get_alltime_maximums(input_folder: str, output_file: str):\n",
    "    # Define the durations and corresponding window sizes\n",
    "    durations = {\n",
    "        \"30-min\": 1,\n",
    "        \"1-hour\": 2,\n",
    "        \"2-hour\": 4,\n",
    "        \"3-hour\": 6,\n",
    "        \"6-hour\": 12,\n",
    "        \"12-hour\": 24,\n",
    "        \"24-hour\": 48,\n",
    "        \"48-hour\": 96,\n",
    "        \"72-hour\": 144\n",
    "    }\n",
    "\n",
    "    # Initialize the output data1\n",
    "    output_rows = []\n",
    "\n",
    "    # Process each file in the input folder\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith(\".csv\") and file_name.startswith(\"ts_\"):\n",
    "            # Extract ID, lat, and lon from file name\n",
    "            parts = file_name.split(\"_\")\n",
    "            station_id = int(parts[1])\n",
    "            lat = float(parts[2])\n",
    "            lon = float(parts[3].replace(\".csv\", \"\"))\n",
    "\n",
    "            # Read the CSV file\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            data = pd.read_csv(file_path)\n",
    "\n",
    "            # Ensure datetime column is parsed and precipitation is numeric\n",
    "            data['time'] = pd.to_datetime(data['time'], errors='coerce')\n",
    "            data['precipitation'] = pd.to_numeric(data['precipitation'], errors='coerce')\n",
    "            data = data.dropna()\n",
    "\n",
    "            # Filter data to include only rows from 2000 to 2024\n",
    "            data = data[(data['time'].dt.year >= 2000) & (data['time'].dt.year <= 2024)]\n",
    "\n",
    "            # Process data for each duration\n",
    "            top_max_values = {duration: [] for duration in durations}\n",
    "            for duration, window_size in durations.items():\n",
    "                # Calculate rolling sums for the specified duration\n",
    "                rolling_sum = data['precipitation'].rolling(window=window_size).sum()\n",
    "                rolling_data = pd.DataFrame({\n",
    "                    \"time\": data['time'][window_size - 1:].reset_index(drop=True),\n",
    "                    \"value\": rolling_sum.dropna().reset_index(drop=True)\n",
    "                })\n",
    "\n",
    "                # Sort by value and exclude overlapping windows\n",
    "                non_overlapping_max = []\n",
    "                used_indices = set()\n",
    "                for idx, row in rolling_data.nlargest(len(rolling_data), 'value').iterrows():\n",
    "                    if all(abs(idx - used_idx) >= window_size for used_idx in used_indices):\n",
    "                        non_overlapping_max.append((row['value'], row['time'].year))\n",
    "                        used_indices.add(idx)\n",
    "                        if len(non_overlapping_max) == 25:\n",
    "                            break\n",
    "\n",
    "                # Store the top 25 maximums for the current duration\n",
    "                top_max_values[duration] = non_overlapping_max\n",
    "\n",
    "            # Create a row for the output\n",
    "            for i in range(25):\n",
    "                output_row = {\n",
    "                    \"ID\": station_id,\n",
    "                    \"lat\": lat,\n",
    "                    \"lon\": lon\n",
    "                }\n",
    "\n",
    "                # Add maximum values and corresponding years for each duration\n",
    "                for duration in durations:\n",
    "                    if i < len(top_max_values[duration]):\n",
    "                        value, year = top_max_values[duration][i]\n",
    "                        output_row[duration] = value\n",
    "                        output_row[f\"year_{duration}\"] = year\n",
    "                    else:\n",
    "                        output_row[duration] = None\n",
    "                        output_row[f\"year_{duration}\"] = None\n",
    "\n",
    "                output_rows.append(output_row)\n",
    "\n",
    "    # Convert output data to DataFrame and save to CSV\n",
    "    output_df = pd.DataFrame(output_rows)\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Input and output paths\n",
    "input_folder = \"output_filesv7\"\n",
    "output_csv = \"alltime_max.csv\"\n",
    "\n",
    "# Run the function\n",
    "get_alltime_maximums(input_folder, output_csv)\n",
    "print(f\"All-time maximums saved to {output_csv}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347002fb-a7f9-4a61-b3df-c3fa83414959",
   "metadata": {},
   "source": [
    "#### **Partial Duration Maximums - (Trimmed None)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3544eef5-5678-483a-9d0c-8a5a8ee7a748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial duration maximums saved to alltime_max_partial_duration.csv\n",
      "Summary saved to alltime_max_partial_duration_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# function to calculate extract partial duration maximums (based on upper and lower bound threshold of annual maximums)\n",
    "\n",
    "def get_partial_duration_maximums(input_folder: str, output_file: str, summary_file: str):\n",
    "    durations = {\n",
    "        \"30-min\": 1,\n",
    "        \"1-hour\": 2,\n",
    "        \"2-hour\": 4,\n",
    "        \"3-hour\": 6,\n",
    "        \"6-hour\": 12,\n",
    "        \"12-hour\": 24,\n",
    "        \"24-hour\": 48,\n",
    "        \"48-hour\": 96,\n",
    "        \"72-hour\": 144\n",
    "    }\n",
    "\n",
    "    all_output_rows = []\n",
    "    summary_rows = []\n",
    "\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith(\".csv\") and file_name.startswith(\"ts_\"):\n",
    "            # Extract metadata from filename\n",
    "            parts = file_name.split(\"_\")\n",
    "            station_id = int(parts[1])\n",
    "            lat = float(parts[2])\n",
    "            lon = float(parts[3].replace(\".csv\", \"\"))\n",
    "\n",
    "            # Read the CSV\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Clean data\n",
    "            df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "            df['precipitation'] = pd.to_numeric(df['precipitation'], errors='coerce')\n",
    "            df = df.dropna().sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "            # Restrict to 2000–2024\n",
    "            df = df[(df['time'].dt.year >= 2000) & (df['time'].dt.year <= 2024)]\n",
    "            if df.empty:\n",
    "                continue\n",
    "\n",
    "            # Step 1: Calculate annual bounds for each duration\n",
    "            df['year'] = df['time'].dt.year\n",
    "            annual_bounds = {}\n",
    "            for duration, window in durations.items():\n",
    "                max_per_year = []\n",
    "                for year, group in df.groupby('year'):\n",
    "                    if group.empty:\n",
    "                        continue\n",
    "                    if window == 1:\n",
    "                        max_val = group['precipitation'].max()\n",
    "                    else:\n",
    "                        max_val = group['precipitation'].rolling(window=window, min_periods=1).sum().max()\n",
    "                    if pd.notna(max_val):\n",
    "                        max_per_year.append(max_val)\n",
    "                if max_per_year:\n",
    "                    annual_bounds[duration] = (min(max_per_year), max(max_per_year))\n",
    "                else:\n",
    "                    annual_bounds[duration] = (None, None)\n",
    "\n",
    "            # Step 2: Collect non-overlapping maxima for each duration\n",
    "            duration_maxima = {}\n",
    "            for duration, window_size in durations.items():\n",
    "                lower, upper = annual_bounds[duration]\n",
    "                if lower is None or upper is None:\n",
    "                    duration_maxima[duration] = []\n",
    "                    continue\n",
    "\n",
    "                rolling_sum = df['precipitation'].rolling(window=window_size).sum()\n",
    "                rolling_data = pd.DataFrame({\n",
    "                    \"time\": df['time'][window_size - 1:].reset_index(drop=True),\n",
    "                    \"value\": rolling_sum.dropna().reset_index(drop=True)\n",
    "                })\n",
    "\n",
    "                # Keep only values within bounds\n",
    "                rolling_data = rolling_data[(rolling_data['value'] >= lower) & (rolling_data['value'] <= upper)]\n",
    "\n",
    "                # Enforce non-overlapping\n",
    "                selected = []\n",
    "                used_idx = set()\n",
    "                for idx, row in rolling_data.nlargest(len(rolling_data), 'value').iterrows():\n",
    "                    if all(abs(idx - u) >= window_size for u in used_idx):\n",
    "                        selected.append((row['value'], row['time'].year))\n",
    "                        used_idx.add(idx)\n",
    "\n",
    "                # Sort descending by value (ensuring year stays aligned)\n",
    "                duration_maxima[duration] = sorted(selected, key=lambda x: x[0], reverse=True)\n",
    "\n",
    "            # Step 3: Align durations — keep only rows where all durations have values\n",
    "            min_len = min(len(v) for v in duration_maxima.values())\n",
    "            for i in range(min_len):  # only keep complete rows\n",
    "                row = {\"ID\": station_id, \"lat\": lat, \"lon\": lon}\n",
    "                for duration in durations:\n",
    "                    val, yr = duration_maxima[duration][i]\n",
    "                    row[duration] = val\n",
    "                    row[f\"year_{duration}\"] = yr\n",
    "                all_output_rows.append(row)\n",
    "\n",
    "            # Step 4: Add summary counts (still store all counts before trimming)\n",
    "            summary_row = {\"ID\": station_id}\n",
    "            for duration in durations:\n",
    "                summary_row[f\"{duration}-count\"] = len(duration_maxima[duration])\n",
    "            summary_rows.append(summary_row)\n",
    "\n",
    "    # Save detailed output (only complete rows kept)\n",
    "    output_df = pd.DataFrame(all_output_rows)\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "\n",
    "    # Save summary file\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "\n",
    "    print(f\"Partial duration maximums saved to {output_file}\")\n",
    "    print(f\"Summary saved to {summary_file}\")\n",
    "\n",
    "\n",
    "# Run Script \n",
    "input_folder = \"output_filesv7\"\n",
    "output_file = \"alltime_partial_duration_max.csv\"\n",
    "summary_file = \"alltime_partial_duration_max_summary.csv\"\n",
    "\n",
    "get_partial_duration_maximums(input_folder, output_file, summary_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afbfe9d-d31b-4f94-ae00-0c69507636eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbf5507-92ef-4352-809f-2251e1068630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda075b7-1c08-455e-b040-85a11ed31f30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df699236-2a30-4ab5-8173-5e7bfebf234a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc6d27f-1920-40dc-b285-aae3d5c0492a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb4c6a4-7566-42fa-90eb-a42c9d46a4f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa1de8-865b-4628-8148-9e27db28b66b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c374844-ef4e-4e92-9882-11f804103d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c6b13d-565e-46a8-9858-dc7d4ab08868",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bafe99e-d499-4191-a2cf-34805a90a025",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4507810-c014-405c-aef5-2f7868667e14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afda401-7ce9-4270-9b0e-70495ab60f19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81a27c7-5bd8-40c3-90bb-7cccc018abd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bc0a10-1520-488e-a4ff-c5d4801bd216",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dc8e6c-c1a0-402e-b8b4-4f85e00c7151",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7c9c9a-b310-4b9d-8a75-749b11286413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51e85cf-6d66-4c2a-9ecc-ce17b3ad523d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481444af-618a-4684-b629-97057b7dd730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8270a0f-96bc-4ab1-9ada-5bba27870621",
   "metadata": {},
   "source": [
    "#### **Partial Duration Maximums - (Complete)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef5b549-3e53-460f-a7cb-f7b9f728e4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partial_duration_maximums(input_folder: str, output_file: str, summary_file: str):\n",
    "    durations = {\n",
    "        \"30-min\": 1,\n",
    "        \"1-hour\": 2,\n",
    "        \"2-hour\": 4,\n",
    "        \"3-hour\": 6,\n",
    "        \"6-hour\": 12,\n",
    "        \"12-hour\": 24,\n",
    "        \"24-hour\": 48,\n",
    "        \"48-hour\": 96,\n",
    "        \"72-hour\": 144\n",
    "    }\n",
    "\n",
    "    output_rows = []\n",
    "    summary_rows = []\n",
    "\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith(\".csv\") and file_name.startswith(\"ts_\"):\n",
    "            # Extract metadata from filename\n",
    "            parts = file_name.split(\"_\")\n",
    "            station_id = int(parts[1])\n",
    "            lat = float(parts[2])\n",
    "            lon = float(parts[3].replace(\".csv\", \"\"))\n",
    "\n",
    "            # Read the CSV\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            df = pd.read_csv(file_path)\n",
    "\n",
    "            # Clean data\n",
    "            df['time'] = pd.to_datetime(df['time'], errors='coerce')\n",
    "            df['precipitation'] = pd.to_numeric(df['precipitation'], errors='coerce')\n",
    "            df = df.dropna().sort_values(\"time\").reset_index(drop=True)\n",
    "\n",
    "            # Restrict to 2000–2024\n",
    "            df = df[(df['time'].dt.year >= 2000) & (df['time'].dt.year <= 2024)]\n",
    "            if df.empty:\n",
    "                continue\n",
    "\n",
    "            # Step 1: Calculate annual bounds for each duration\n",
    "            df['year'] = df['time'].dt.year\n",
    "            annual_bounds = {}\n",
    "            for duration, window in durations.items():\n",
    "                max_per_year = []\n",
    "                for year, group in df.groupby('year'):\n",
    "                    if group.empty:\n",
    "                        continue\n",
    "                    if window == 1:\n",
    "                        max_val = group['precipitation'].max()\n",
    "                    else:\n",
    "                        max_val = group['precipitation'].rolling(window=window, min_periods=1).sum().max()\n",
    "                    if pd.notna(max_val):\n",
    "                        max_per_year.append(max_val)\n",
    "                if max_per_year:\n",
    "                    annual_bounds[duration] = (min(max_per_year), max(max_per_year))\n",
    "                else:\n",
    "                    annual_bounds[duration] = (None, None)\n",
    "\n",
    "            # Step 2: Collect non-overlapping maxima for each duration\n",
    "            duration_maxima = {}\n",
    "            for duration, window_size in durations.items():\n",
    "                lower, upper = annual_bounds[duration]\n",
    "                if lower is None or upper is None:\n",
    "                    duration_maxima[duration] = []\n",
    "                    continue\n",
    "\n",
    "                rolling_sum = df['precipitation'].rolling(window=window_size).sum()\n",
    "                rolling_data = pd.DataFrame({\n",
    "                    \"time\": df['time'][window_size - 1:].reset_index(drop=True),\n",
    "                    \"value\": rolling_sum.dropna().reset_index(drop=True)\n",
    "                })\n",
    "\n",
    "                # Keep only values within bounds\n",
    "                rolling_data = rolling_data[(rolling_data['value'] >= lower) & (rolling_data['value'] <= upper)]\n",
    "\n",
    "                # Enforce non-overlapping\n",
    "                selected = []\n",
    "                used_idx = set()\n",
    "                for idx, row in rolling_data.nlargest(len(rolling_data), 'value').iterrows():\n",
    "                    if all(abs(idx - u) >= window_size for u in used_idx):\n",
    "                        selected.append((row['value'], row['time'].year))\n",
    "                        used_idx.add(idx)\n",
    "\n",
    "                duration_maxima[duration] = selected\n",
    "\n",
    "            # Step 3: Create wide-format rows\n",
    "            max_len = max(len(v) for v in duration_maxima.values())\n",
    "            for i in range(max_len):\n",
    "                row = {\"ID\": station_id, \"lat\": lat, \"lon\": lon}\n",
    "                for duration in durations:\n",
    "                    if i < len(duration_maxima[duration]):\n",
    "                        val, yr = duration_maxima[duration][i]\n",
    "                        row[duration] = val\n",
    "                        row[f\"year_{duration}\"] = yr\n",
    "                    else:\n",
    "                        row[duration] = None\n",
    "                        row[f\"year_{duration}\"] = None\n",
    "                output_rows.append(row)\n",
    "\n",
    "            # Step 4: Add counts for summary file\n",
    "            summary_row = {\"ID\": station_id}\n",
    "            for duration in durations:\n",
    "                summary_row[f\"{duration}-count\"] = len(duration_maxima[duration])\n",
    "            summary_rows.append(summary_row)\n",
    "\n",
    "    # Save detailed output (wide format)\n",
    "    output_df = pd.DataFrame(output_rows)\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "\n",
    "    # Save summary file (with -count suffix)\n",
    "    summary_df = pd.DataFrame(summary_rows)\n",
    "    summary_df.to_csv(summary_file, index=False)\n",
    "\n",
    "    print(f\"Partial duration maximums saved to {output_file}\")\n",
    "    print(f\"Summary saved to {summary_file}\")\n",
    "\n",
    "\n",
    "# Run Script \n",
    "input_folder = \"output_filesv7\"\n",
    "output_file = \"alltime_partial_duration_max_raw.csv\"\n",
    "summary_file = \"alltime_partial_duration_max_raw_summary.csv\"\n",
    "\n",
    "get_partial_duration_maximums(input_folder, output_file, summary_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ca1c24-cf68-422a-af18-9eb78f58a12e",
   "metadata": {},
   "source": [
    "#### **Extras (Alltime Maximums - Old Approach)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b46dc11-d2df-419c-bebc-fed383061316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract all-time maximums within the date range 2001 to 2023\n",
    "def get_alltime_maximums(input_folder: str, output_file: str):\n",
    "    # Define the durations and corresponding window sizes\n",
    "    durations = {\n",
    "        \"30-min\": 1,\n",
    "        \"1-hour\": 2,\n",
    "        \"2-hour\": 4,\n",
    "        \"3-hour\": 6,\n",
    "        \"6-hour\": 12,\n",
    "        \"12-hour\": 24,\n",
    "        \"24-hour\": 48,\n",
    "        \"48-hour\": 96,\n",
    "        \"72-hour\": 144\n",
    "    }\n",
    "\n",
    "    # Initialize the output DataFrame\n",
    "    output_rows = []\n",
    "\n",
    "    # Process each file in the input folder\n",
    "    for file_name in os.listdir(input_folder):\n",
    "        if file_name.endswith(\".csv\") and file_name.startswith(\"ts_\"):\n",
    "            # Extract ID, lat, and lon from file name\n",
    "            parts = file_name.split(\"_\")\n",
    "            station_id = int(parts[1])\n",
    "            lat = float(parts[2])\n",
    "            lon = float(parts[3].replace(\".csv\", \"\"))\n",
    "\n",
    "            # Read the CSV file\n",
    "            file_path = os.path.join(input_folder, file_name)\n",
    "            data = pd.read_csv(file_path)\n",
    "            \n",
    "            # Ensure datetime column is parsed and precipitation is numeric\n",
    "            data['time'] = pd.to_datetime(data['time'], errors='coerce')\n",
    "            data['precipitationCal'] = pd.to_numeric(data['precipitationCal'], errors='coerce')\n",
    "            data = data.dropna()\n",
    "\n",
    "            # Filter data to only include rows from 2001 to 2023\n",
    "            data = data[(data['time'].dt.year >= 2001) & (data['time'].dt.year <= 2023)]\n",
    "\n",
    "            # Create a dictionary to store the top 23 values for each duration\n",
    "            top_max_values = {duration: [] for duration in durations}\n",
    "\n",
    "            # Process data for each duration\n",
    "            for duration, window_size in durations.items():\n",
    "                # Calculate rolling sums for the specified duration\n",
    "                rolling_sum = data['precipitationCal'].rolling(window=window_size).sum()\n",
    "                rolling_data = pd.DataFrame({\n",
    "                    \"time\": data['time'],\n",
    "                    \"value\": rolling_sum\n",
    "                }).dropna()\n",
    "                \n",
    "                # Extract top 23 maximums\n",
    "                top_max = rolling_data.nlargest(23, 'value')\n",
    "\n",
    "                # Store the top 23 values for the current duration\n",
    "                top_max_values[duration] = top_max\n",
    "\n",
    "            # Create 23 rows, one for each highest value\n",
    "            for i in range(23):\n",
    "                output_row = {\n",
    "                    \"ID\": station_id,\n",
    "                    \"lat\": lat,\n",
    "                    \"lon\": lon\n",
    "                }\n",
    "\n",
    "                # For each duration, get the i-th highest value and the corresponding year\n",
    "                for duration in durations:\n",
    "                    if i < len(top_max_values[duration]):\n",
    "                        top_value = top_max_values[duration].iloc[i]\n",
    "                        output_row[duration] = top_value['value']\n",
    "                        output_row[f\"year_{duration}\"] = top_value['time'].year\n",
    "                    else:\n",
    "                        output_row[duration] = None\n",
    "                        output_row[f\"year_{duration}\"] = None\n",
    "\n",
    "                # Append the row to the output list\n",
    "                output_rows.append(output_row)\n",
    "\n",
    "    # Combine all rows into a single DataFrame\n",
    "    output_df = pd.DataFrame(output_rows)\n",
    "\n",
    "    # Write to output CSV\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Input and output paths\n",
    "imerg_folder = \"output_files\"\n",
    "output_csv = \"alltime_max_2001_2023.csv\"\n",
    "\n",
    "# Run Function\n",
    "get_alltime_maximums(imerg_folder, output_csv)\n",
    "\n",
    "print(f\"All-time maximums (Y2001 to Y2023) saved to {output_csv}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3ff872-5d07-4fc4-ba37-46f03d283282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
