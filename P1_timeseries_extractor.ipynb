{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1256cf0-383c-4dad-82e1-f3f7d098b62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    This notebook extracts IMERG Timeseries Precipitations for Multi-stations. \n",
    "    \n",
    "    contact\n",
    "    ----------\n",
    "    Dr. KENNETH EKPETERE | kenneth.ekpetere@gmail.com\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8f2701-ef43-444f-ab3a-c967731f766e",
   "metadata": {},
   "source": [
    "### **Extract IMERG Precipitation for all Station points/pixels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49911af6-9771-477b-b6ef-2d726843196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geemap\n",
    "import ee\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import csv\n",
    "# import time as tm\n",
    "# from datetime import timedelta, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f770c5c-9638-4e8e-a29e-a00355032d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Authenticate Earth Engine.\n",
    "# ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c35a8fbb-13af-4b74-b92f-0d166c402b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Earth Engine module.\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "968fab03-16e2-4e35-9ce9-4a8b69d3f32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ID: 238003. Saved to output_filesv7B/ts_238003_40.22_-94.5444.csv\n",
      "Processed ID: 237967. Saved to output_filesv7B/ts_237967_36.9839_-94.5356.csv\n",
      "Processed ID: 221962. Saved to output_filesv7B/ts_221962_34.9175_-88.5228.csv\n",
      "Processed ID: 215298. Saved to output_filesv7B/ts_215298_46.9833_-92.7333.csv\n",
      "Processed ID: 135796. Saved to output_filesv7B/ts_135796_40.9486_-91.5647.csv\n",
      "Processed ID: 130200. Saved to output_filesv7B/ts_130200_42.0208_-93.7742.csv\n",
      "Processed ID: 5670. Saved to output_filesv7B/ts_5670_33.965_-112.4286.csv\n",
      "All IDs processed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to extract IMERG time series for a given station/pixel\n",
    "def extract_imerg_time_series(lat, lon):\n",
    "    # IMERG dataset\n",
    "    # dataset = ee.ImageCollection('NASA/GPM_L3/IMERG_V06').select('precipitationCal') # v06 (2000 - 2023)\n",
    "\n",
    "    # v07 => https://developers.google.com/earth-engine/datasets/catalog/NASA_GPM_L3_IMERG_V07#bands\n",
    "    dataset = ee.ImageCollection('NASA/GPM_L3/IMERG_V07').select('precipitation') # v07 (2000 - 2024) \n",
    "    \n",
    "\n",
    "    # Define point of interest from stn list\n",
    "    point = ee.Geometry.Point(lon, lat)\n",
    "\n",
    "    # Initialize empty list to store yearly dataframes\n",
    "    yearly_dfs = []\n",
    "\n",
    "    # Define start and end dates for yearly chunks\n",
    "    start_date = ee.Date('2000-01-01')\n",
    "    end_date = ee.Date('2024-12-31')  # increased year from 2023 to 2024 for v07\n",
    "\n",
    "    # Iterate over years and extract data in yearly chunks\n",
    "    year = start_date.get('year')\n",
    "    while year.getInfo() <= end_date.get('year').getInfo():\n",
    "        # Define current year's date range\n",
    "        start_year = ee.Date.fromYMD(year, 1, 1)\n",
    "        end_year = ee.Date.fromYMD(year, 12, 31)\n",
    "        end_year = end_year.advance(1, 'day')\n",
    "\n",
    "        # Filter dataset by current year\n",
    "        filtered = dataset.filterDate(start_year, end_year)\n",
    "\n",
    "        # Extract time-series at the point\n",
    "        ts = filtered.getRegion(point, scale=11132).getInfo()\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(ts[1:], columns=ts[0])\n",
    "        # df = df[['time', 'precipitationCal']] # uncomment to run v06\n",
    "        df = df[['time', 'precipitation']] # comment to run v07\n",
    "        df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "\n",
    "        # Append yearly DataFrame to list\n",
    "        yearly_dfs.append(df)\n",
    "\n",
    "        # Move to the next year\n",
    "        year = ee.Number(year).add(1)\n",
    "\n",
    "    # Concatenate all yearly dataframes into one\n",
    "    combined_df = pd.concat(yearly_dfs, ignore_index=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Read input CSV file\n",
    "# input_file = 'stn.csv'  # full stations (2360 - stations)\n",
    "input_file = 'stn_tt.csv'     # test stations\n",
    "output_folder = 'output_filesv7_test/'\n",
    "\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "# Process each row in the CSV\n",
    "for index, row in data.iterrows():\n",
    "    unique_id = str(int(row['ID']))  # Convert ID to string\n",
    "\n",
    "    try:\n",
    "        lat = row['Lat']\n",
    "        lon = row['Lon']\n",
    "\n",
    "        # Extract IMERG time series\n",
    "        ts_df = extract_imerg_time_series(lat, lon)\n",
    "\n",
    "        # Save output to CSV\n",
    "        filename = f\"ts_{unique_id}_{lat}_{lon}.csv\"\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        ts_df.to_csv(output_path, index=False)\n",
    "\n",
    "        print(f\"Processed ID: {unique_id}. Saved to {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ID {unique_id}: {str(e)}\")\n",
    "\n",
    "    # Pause for 5 seconds to prevent memory issues and respect GEE limitations\n",
    "    time.sleep(5)\n",
    "\n",
    "print(\"All IDs processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6838fd7e-d196-4780-b1c3-47400d85dc8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7d1da3f2-ef17-4c57-8bd2-da7336d4f582",
   "metadata": {},
   "source": [
    "#### **Retrieve December 31 Alone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9af89adf-2525-4dae-a685-e5dd2220e3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed ID: 930168. Saved to output_filesv7B/ts_930168_34.2016_-119.20700000000001.csv\n",
      "Processed ID: 790061. Saved to output_filesv7B/ts_790061_29.98_-95.36.csv\n",
      "Processed ID: 413415. Saved to output_filesv7B/ts_413415_33.6358_-97.1447.csv\n",
      "Processed ID: 143984. Saved to output_filesv7B/ts_143984_37.9233_-95.4242.csv\n",
      "Processed ID: 143810. Saved to output_filesv7B/ts_143810_39.6703_-95.5225.csv\n",
      "Processed ID: 113290. Saved to output_filesv7B/ts_113290_41.8981_-90.1539.csv\n",
      "Processed ID: 113262. Saved to output_filesv7B/ts_113262_42.2953_-89.6061.csv\n",
      "Processed ID: 113109. Saved to output_filesv7B/ts_113109_38.7156_-88.5822.csv\n",
      "Processed ID: 53488. Saved to output_filesv7B/ts_53488_39.1342_-108.5375.csv\n",
      "Processed ID: 53477. Saved to output_filesv7B/ts_53477_38.0611_-102.3111.csv\n",
      "Processed ID: 43791. Saved to output_filesv7B/ts_43791_40.3636_-122.965.csv\n",
      "Processed ID: 43761. Saved to output_filesv7B/ts_43761_41.8042_-123.3758.csv\n",
      "Processed ID: 15478. Saved to output_filesv7B/ts_15478_30.6883_-88.2456.csv\n",
      "Processed ID: 15397. Saved to output_filesv7B/ts_15397_32.0597_-85.4953.csv\n",
      "All IDs processed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Function to extract IMERG time series for a given station/pixel (dec 31 of every year alone)\n",
    "def extract_imerg_time_series(lat, lon):\n",
    "    # IMERG dataset\n",
    "    # dataset = ee.ImageCollection('NASA/GPM_L3/IMERG_V06').select('precipitationCal') # v06 (2000 - 2023)\n",
    "\n",
    "    # v07 => https://developers.google.com/earth-engine/datasets/catalog/NASA_GPM_L3_IMERG_V07#bands\n",
    "    dataset = ee.ImageCollection('NASA/GPM_L3/IMERG_V07').select('precipitation') # v07 (2000 - 2024) \n",
    "    \n",
    "\n",
    "    # Define point of interest from stn list\n",
    "    point = ee.Geometry.Point(lon, lat)\n",
    "\n",
    "    # Initialize empty list to store yearly dataframes\n",
    "    yearly_dfs = []\n",
    "\n",
    "    # Define start and end dates for yearly chunks\n",
    "    start_date = ee.Date('2000-01-01')\n",
    "    end_date = ee.Date('2024-12-31')  # increased year from 2023 to 2024 for v07\n",
    "\n",
    "    # Iterate over years and extract data in yearly chunks\n",
    "    year = start_date.get('year')\n",
    "    while year.getInfo() <= end_date.get('year').getInfo():\n",
    "        \n",
    "        # Define current year's date range\n",
    "        dec31_start_date = ee.Date.fromYMD(year, 12, 31)\n",
    "        dec31_end_date = dec31_start_date.advance(1, 'day')\n",
    "\n",
    "        # Filter dataset by current year\n",
    "        filtered = dataset.filterDate(dec31_start_date, dec31_end_date)\n",
    "\n",
    "        # Extract time-series at the point\n",
    "        ts = filtered.getRegion(point, scale=11132).getInfo()\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(ts[1:], columns=ts[0])\n",
    "        # df = df[['time', 'precipitationCal']] # uncomment to run v06\n",
    "        df = df[['time', 'precipitation']] # comment to run v07\n",
    "        df['time'] = pd.to_datetime(df['time'], unit='ms')\n",
    "\n",
    "        # Append yearly DataFrame to list\n",
    "        yearly_dfs.append(df)\n",
    "\n",
    "        # Move to the next year\n",
    "        year = ee.Number(year).add(1)\n",
    "\n",
    "    # Concatenate all yearly dataframes into one\n",
    "    combined_df = pd.concat(yearly_dfs, ignore_index=True)\n",
    "\n",
    "    return combined_df\n",
    "\n",
    "# Read input CSV file\n",
    "# input_file = 'stn.csv'  # full stations (2360 - stations)\n",
    "input_file = 'stn_tt.csv'     # test - stations\n",
    "output_folder = 'output_filesv7B/'\n",
    "\n",
    "data = pd.read_csv(input_file)\n",
    "\n",
    "# Process each row in the CSV\n",
    "for index, row in data.iterrows():\n",
    "    unique_id = str(int(row['ID']))  # Convert ID to string\n",
    "\n",
    "    try:\n",
    "        lat = row['Lat']\n",
    "        lon = row['Lon']\n",
    "\n",
    "        # Extract IMERG time series\n",
    "        ts_df = extract_imerg_time_series(lat, lon)\n",
    "\n",
    "        # Save output to CSV\n",
    "        filename = f\"ts_{unique_id}_{lat}_{lon}.csv\"\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "        ts_df.to_csv(output_path, index=False)\n",
    "\n",
    "        print(f\"Processed ID: {unique_id}. Saved to {output_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing ID {unique_id}: {str(e)}\")\n",
    "\n",
    "    # Pause for 5 seconds to prevent memory issues and respect GEE limitations\n",
    "    time.sleep(5)\n",
    "\n",
    "print(\"All IDs processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de97f2-e6f8-4971-a797-1b8126f3525c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e46ffdc3-10c0-4322-abff-0dc2eb7148fe",
   "metadata": {},
   "source": [
    "#### **Merge december 31 to main df**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3729da4a-1216-4141-a5a1-e30c7e94e48b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All IDs merged completed.\n"
     ]
    }
   ],
   "source": [
    "# Function to merge december 31 of every year to the main dataframe\n",
    "def merge_dataframes(folder_v7A, folder_v7B, output_folder):\n",
    "    # Ensure output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Get the list of files from both folders\n",
    "    files_v7A = {f: os.path.join(folder_v7A, f) for f in os.listdir(folder_v7A) if f.endswith('.csv')}\n",
    "    files_v7B = {f: os.path.join(folder_v7B, f) for f in os.listdir(folder_v7B) if f.endswith('.csv')}\n",
    "\n",
    "    for file_name, path_v7A in files_v7A.items():\n",
    "        # Extract the ID from the file name\n",
    "        id_v7A = file_name.split('_')[1]\n",
    "\n",
    "        # Check if a corresponding file exists in folder_v7B\n",
    "        corresponding_file = next((f for f in files_v7B if f.split('_')[1] == id_v7A), None)\n",
    "\n",
    "        if corresponding_file:\n",
    "            path_v7B = files_v7B[corresponding_file]\n",
    "\n",
    "            # Read the data from both files\n",
    "            df_v7A = pd.read_csv(path_v7A)\n",
    "            df_v7B = pd.read_csv(path_v7B)\n",
    "\n",
    "            # Merge the dataframes\n",
    "            merged_df = pd.concat([df_v7A, df_v7B])\n",
    "\n",
    "            # Sort the merged dataframe by 'time'\n",
    "            merged_df['time'] = pd.to_datetime(merged_df['time'])  # Ensure time is datetime\n",
    "            merged_df.sort_values(by='time', inplace=True)\n",
    "\n",
    "            # Write out the merged dataframe to the output folder\n",
    "            output_path = os.path.join(output_folder, file_name)\n",
    "            merged_df.to_csv(output_path, index=False)\n",
    "\n",
    "\n",
    "# Run Function\n",
    "folder_v7A = \"output_filesv7A\"\n",
    "folder_v7B = \"output_filesv7B\"\n",
    "output_folder = \"output_filesv7\"\n",
    "\n",
    "merge_dataframes(folder_v7A, folder_v7B, output_folder)\n",
    "print(\"All IDs merged completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d8af75-b852-417b-8f15-5d9daa1950ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "284bc8a2-b62b-4660-922d-757d66f7750c",
   "metadata": {},
   "source": [
    "#### **Count Records**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed30fa66-0d06-49f9-9411-a48db95fa54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Checks completed.\n"
     ]
    }
   ],
   "source": [
    "# Function to check df record length\n",
    "def count_df_records(imerg_folder, output_csv):\n",
    "    # Define the header for the output CSV file\n",
    "    output_data = [['ID', 'lat', 'lon', 'record', 'diff', 'start_datetime', 'end_datetime']]\n",
    "\n",
    "    # Iterate through each file in the folder\n",
    "    for file_name in os.listdir(imerg_folder):\n",
    "        if file_name.endswith('.csv'):\n",
    "            # Parse the file name to extract ID, lat, and lon\n",
    "            parts = file_name.split('_')\n",
    "            if len(parts) != 4 or not parts[1].isdigit():\n",
    "                print(f\"Skipping invalid file name format: {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            station_id = parts[1]\n",
    "            lat = parts[2]\n",
    "            lon = parts[3].replace('.csv', '')\n",
    "\n",
    "            # Construct the full file path\n",
    "            file_path = os.path.join(imerg_folder, file_name)\n",
    "\n",
    "            # Read the file and count the rows using the 'time' field\n",
    "            try:\n",
    "                df = pd.read_csv(file_path)\n",
    "                record_count = len(df['time'])\n",
    "\n",
    "                # Extract the first and last 'time' records\n",
    "                start_datetime = df['time'].iloc[0] if not df.empty else None\n",
    "                end_datetime = df['time'].iloc[-1] if not df.empty else None\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {file_name}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # Calculate the diff value\n",
    "            diff = 438333 - record_count\n",
    "\n",
    "            # Append the data to the output list\n",
    "            output_data.append([station_id, lat, lon, record_count, diff, start_datetime, end_datetime])\n",
    "\n",
    "    # Write the output data to the CSV file\n",
    "    with open(output_csv, mode='w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        writer.writerows(output_data)\n",
    "\n",
    "# Run Function\n",
    "Imerg_folder = \"output_filesv7\"\n",
    "output_csv = \"Imerg_data_check.csv\"\n",
    "count_df_records(Imerg_folder, output_csv)\n",
    "print(\"File Checks completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139ada1b-a3f1-48f8-8bf6-8f150ec909c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
